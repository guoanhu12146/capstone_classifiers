{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guoanhu/.pyenv/versions/3.6.14/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20965 unique tokens.\n",
      "Found 1193515 word vectors.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/hate_classes.tsv\", sep='\\t')\n",
    "# process text for embeddings\n",
    "text = df.Text.apply(basic_tokenize)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000, split=' ', oov_token='<unw>', filters='')\n",
    "tokenizer.fit_on_texts(text.values)\n",
    "X = tokenizer.texts_to_sequences(text.values)\n",
    "X = pad_sequences(X, maxlen=200)\n",
    "\n",
    "voc = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(voc))\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('./', 'glove.twitter.27B.200d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "num_words = min(max_features, len(voc))\n",
    "print(num_words)\n",
    "\n",
    "embedding_dim = 200\n",
    "\n",
    "# first create a matrix of zeros, this is our embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in voc.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = []\n",
    "for row in df.itertuples():\n",
    "    Y.append((row.RAE,row.NAT,row.SXO,row.GEN,row.REL,row.IDL,row.POL,row.MPH))\n",
    "Y = np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/guoanhu/.pyenv/versions/3.6.14/lib/python3.6/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/guoanhu/.pyenv/versions/3.6.14/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 200)          4000000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 200, 128)          168448    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 264       \n",
      "=================================================================\n",
      "Total params: 4,220,200\n",
      "Trainable params: 220,200\n",
      "Non-trainable params: 4,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, Embedding, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "#Defining Neural Network\n",
    "model = Sequential()\n",
    "#Non-trainable embeddidng layer\n",
    "model.add(Embedding(max_features, output_dim=200, weights=[embedding_matrix], input_length=200, trainable=False))\n",
    "#LSTM \n",
    "model.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\n",
    "model.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\n",
    "model.add(Dense(units = 32 , activation = 'relu'))\n",
    "model.add(Dense(8, activation='sigmoid'))\n",
    "model.compile(optimizer=RMSprop(lr = 0.001), loss='categorical_crossentropy', metrics=['binary_accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 200)          4000000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 193, 64)           102464    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 96, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6144)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 40)                245800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 328       \n",
      "=================================================================\n",
      "Total params: 4,348,592\n",
      "Trainable params: 348,592\n",
      "Non-trainable params: 4,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, output_dim=200, weights=[embedding_matrix], input_length=200, trainable=False))\n",
    "model.add(Conv1D(filters=64, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dense(8, activation='sigmoid'))\n",
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr = 0.001), metrics=['binary_accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/guoanhu/.pyenv/versions/3.6.14/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 4371 samples, validate on 486 samples\n",
      "Epoch 1/30\n",
      "4371/4371 [==============================] - 4s 807us/sample - loss: 2.2835 - binary_accuracy: 0.5147 - val_loss: 2.2571 - val_binary_accuracy: 0.4954\n",
      "Epoch 2/30\n",
      "4371/4371 [==============================] - 0s 93us/sample - loss: 2.1161 - binary_accuracy: 0.6740 - val_loss: 2.2126 - val_binary_accuracy: 0.7623\n",
      "Epoch 3/30\n",
      "4371/4371 [==============================] - 0s 88us/sample - loss: 1.8120 - binary_accuracy: 0.8433 - val_loss: 2.3265 - val_binary_accuracy: 0.8413\n",
      "Epoch 4/30\n",
      "4371/4371 [==============================] - 0s 84us/sample - loss: 1.4678 - binary_accuracy: 0.8639 - val_loss: 2.7073 - val_binary_accuracy: 0.8467\n",
      "Epoch 5/30\n",
      "4371/4371 [==============================] - 0s 85us/sample - loss: 1.2074 - binary_accuracy: 0.8613 - val_loss: 2.6245 - val_binary_accuracy: 0.8467\n",
      "Epoch 6/30\n",
      "4371/4371 [==============================] - 0s 82us/sample - loss: 1.0464 - binary_accuracy: 0.8679 - val_loss: 2.6869 - val_binary_accuracy: 0.8513\n",
      "Epoch 7/30\n",
      "4371/4371 [==============================] - 0s 85us/sample - loss: 0.9120 - binary_accuracy: 0.8765 - val_loss: 3.4534 - val_binary_accuracy: 0.8501\n",
      "Epoch 8/30\n",
      "4371/4371 [==============================] - 0s 88us/sample - loss: 0.8260 - binary_accuracy: 0.8918 - val_loss: 2.9420 - val_binary_accuracy: 0.8524\n",
      "Epoch 9/30\n",
      "4371/4371 [==============================] - 0s 90us/sample - loss: 0.7656 - binary_accuracy: 0.9059 - val_loss: 3.3699 - val_binary_accuracy: 0.8521\n",
      "Epoch 10/30\n",
      "4371/4371 [==============================] - 0s 84us/sample - loss: 0.6853 - binary_accuracy: 0.9186 - val_loss: 3.5177 - val_binary_accuracy: 0.8436\n",
      "Epoch 11/30\n",
      "4371/4371 [==============================] - 0s 82us/sample - loss: 0.6402 - binary_accuracy: 0.9332 - val_loss: 3.4229 - val_binary_accuracy: 0.8519\n",
      "Epoch 12/30\n",
      "4371/4371 [==============================] - 0s 82us/sample - loss: 0.5895 - binary_accuracy: 0.9417 - val_loss: 3.6767 - val_binary_accuracy: 0.8508\n",
      "Epoch 13/30\n",
      "4371/4371 [==============================] - 0s 86us/sample - loss: 0.5589 - binary_accuracy: 0.9495 - val_loss: 3.8474 - val_binary_accuracy: 0.8506\n",
      "Epoch 14/30\n",
      "4371/4371 [==============================] - 0s 87us/sample - loss: 0.5331 - binary_accuracy: 0.9576 - val_loss: 4.0298 - val_binary_accuracy: 0.8547\n",
      "Epoch 15/30\n",
      "4371/4371 [==============================] - 0s 87us/sample - loss: 0.5103 - binary_accuracy: 0.9646 - val_loss: 4.0950 - val_binary_accuracy: 0.8544\n",
      "Epoch 16/30\n",
      "4371/4371 [==============================] - 0s 86us/sample - loss: 0.4814 - binary_accuracy: 0.9669 - val_loss: 4.3641 - val_binary_accuracy: 0.8519\n",
      "Epoch 17/30\n",
      "4371/4371 [==============================] - 0s 83us/sample - loss: 0.4807 - binary_accuracy: 0.9734 - val_loss: 4.7203 - val_binary_accuracy: 0.8495\n",
      "Epoch 18/30\n",
      "4371/4371 [==============================] - 0s 86us/sample - loss: 0.4629 - binary_accuracy: 0.9746 - val_loss: 4.8569 - val_binary_accuracy: 0.8503\n",
      "Epoch 19/30\n",
      "4371/4371 [==============================] - 0s 86us/sample - loss: 0.4511 - binary_accuracy: 0.9752 - val_loss: 4.8827 - val_binary_accuracy: 0.8516\n",
      "Epoch 20/30\n",
      "4371/4371 [==============================] - 0s 83us/sample - loss: 0.4402 - binary_accuracy: 0.9785 - val_loss: 5.2168 - val_binary_accuracy: 0.8516\n",
      "Epoch 21/30\n",
      "4371/4371 [==============================] - 0s 86us/sample - loss: 0.4325 - binary_accuracy: 0.9788 - val_loss: 5.5008 - val_binary_accuracy: 0.8477\n",
      "Epoch 22/30\n",
      "4371/4371 [==============================] - 0s 88us/sample - loss: 0.4344 - binary_accuracy: 0.9814 - val_loss: 5.4818 - val_binary_accuracy: 0.8495\n",
      "Epoch 23/30\n",
      "4371/4371 [==============================] - 0s 83us/sample - loss: 0.4296 - binary_accuracy: 0.9817 - val_loss: 5.4805 - val_binary_accuracy: 0.8519\n",
      "Epoch 24/30\n",
      "4371/4371 [==============================] - 0s 84us/sample - loss: 0.4162 - binary_accuracy: 0.9815 - val_loss: 5.8332 - val_binary_accuracy: 0.8552\n",
      "Epoch 25/30\n",
      "4371/4371 [==============================] - 0s 85us/sample - loss: 0.4217 - binary_accuracy: 0.9818 - val_loss: 5.8397 - val_binary_accuracy: 0.8503\n",
      "Epoch 26/30\n",
      "4371/4371 [==============================] - 0s 84us/sample - loss: 0.4138 - binary_accuracy: 0.9828 - val_loss: 6.0752 - val_binary_accuracy: 0.8477\n",
      "Epoch 27/30\n",
      "4371/4371 [==============================] - 0s 89us/sample - loss: 0.4076 - binary_accuracy: 0.9826 - val_loss: 6.1730 - val_binary_accuracy: 0.8459\n",
      "Epoch 28/30\n",
      "4371/4371 [==============================] - 0s 93us/sample - loss: 0.4121 - binary_accuracy: 0.9836 - val_loss: 6.3303 - val_binary_accuracy: 0.8475\n",
      "Epoch 29/30\n",
      "4371/4371 [==============================] - 0s 88us/sample - loss: 0.4086 - binary_accuracy: 0.9837 - val_loss: 6.4402 - val_binary_accuracy: 0.8495\n",
      "Epoch 30/30\n",
      "4371/4371 [==============================] - 0s 85us/sample - loss: 0.4085 - binary_accuracy: 0.9853 - val_loss: 7.1534 - val_binary_accuracy: 0.8449\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=42, test_size=0.1)\n",
    "\n",
    "batch_size = 64  # mini-batch with 256 examples\n",
    "epochs = 30\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“These [companies] are run by sociopaths,” he said. “These people are complete narcissists. These people ought to be controlled, they ought to be regulated.” At one point during the phone call, Bannon said, “These people are evil.' https://www.vanityfair.com/news/2018/08/steve-bann...\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "a = model.predict(x_test[1].reshape(1,200))\n",
    "a = np.round(a)\n",
    "print(df.loc[1,:].Text)\n",
    "print(a)\n",
    "print(y_test[1])\n",
    "# (row.RAE,row.NAT,row.SXO,row.GEN,row.REL,row.IDL,row.POL,row.MPH)\n",
    "model.save('../models/hate_classification.h5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9839dc81feffa0ef6b0305e2f8d6f48bfea3278be234a4dfafb21f6ff94f6b4b"
  },
  "kernelspec": {
   "display_name": "Python 3.6.14 64-bit ('3.6.14')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
