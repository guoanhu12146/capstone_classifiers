{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20965 unique tokens.\n",
      "Found 1193515 word vectors.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/hate_classes.tsv\", sep='\\t')\n",
    "# process text for embeddings\n",
    "text = df.Text.apply(basic_tokenize)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000, split=' ', oov_token='<unw>', filters='')\n",
    "tokenizer.fit_on_texts(text.values)\n",
    "X = tokenizer.texts_to_sequences(text.values)\n",
    "X = pad_sequences(X, maxlen=200)\n",
    "\n",
    "voc = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(voc))\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('./', 'glove.twitter.27B.200d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "num_words = min(max_features, len(voc))\n",
    "print(num_words)\n",
    "\n",
    "embedding_dim = 200\n",
    "\n",
    "# first create a matrix of zeros, this is our embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in voc.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = []\n",
    "for row in df.itertuples():\n",
    "    Y.append((row.REL,row.RAE,row.SXO,row.GEN,row.IDL,row.NAT,row.POL,row.MPH,row.EX,row.IM))\n",
    "Y = np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_30 (Embedding)     (None, 200, 200)          4000000   \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 200, 128)          168448    \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 4,220,266\n",
      "Trainable params: 220,266\n",
      "Non-trainable params: 4,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, Embedding, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "#Defining Neural Network\n",
    "model = Sequential()\n",
    "#Non-trainable embeddidng layer\n",
    "model.add(Embedding(max_features, output_dim=200, weights=[embedding_matrix], input_length=200, trainable=False))\n",
    "#LSTM \n",
    "model.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\n",
    "model.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\n",
    "model.add(Dense(units = 32 , activation = 'relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.compile(optimizer=RMSprop(lr = 0.001), loss='categorical_crossentropy', metrics=['binary_accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_34 (Embedding)     (None, 200, 200)          4000000   \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 193, 64)           102464    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 96, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 6144)              0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 40)                245800    \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 10)                410       \n",
      "=================================================================\n",
      "Total params: 4,348,674\n",
      "Trainable params: 348,674\n",
      "Non-trainable params: 4,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, output_dim=200, weights=[embedding_matrix], input_length=200, trainable=False))\n",
    "model.add(Conv1D(filters=64, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr = 0.001), metrics=['binary_accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5830 samples, validate on 648 samples\n",
      "Epoch 1/30\n",
      "5830/5830 [==============================] - 2s 298us/sample - loss: 4.2778 - binary_accuracy: 0.6894 - val_loss: 4.2191 - val_binary_accuracy: 0.7349\n",
      "Epoch 2/30\n",
      "5830/5830 [==============================] - 1s 109us/sample - loss: 4.0067 - binary_accuracy: 0.7810 - val_loss: 4.1779 - val_binary_accuracy: 0.7106\n",
      "Epoch 3/30\n",
      "5830/5830 [==============================] - 1s 116us/sample - loss: 3.6722 - binary_accuracy: 0.8186 - val_loss: 4.0849 - val_binary_accuracy: 0.7613\n",
      "Epoch 4/30\n",
      "5830/5830 [==============================] - 1s 107us/sample - loss: 3.3715 - binary_accuracy: 0.8423 - val_loss: 4.3534 - val_binary_accuracy: 0.7835\n",
      "Epoch 5/30\n",
      "5830/5830 [==============================] - 1s 115us/sample - loss: 3.1407 - binary_accuracy: 0.8589 - val_loss: 4.5371 - val_binary_accuracy: 0.7921\n",
      "Epoch 6/30\n",
      "5830/5830 [==============================] - 1s 109us/sample - loss: 2.9601 - binary_accuracy: 0.8712 - val_loss: 4.4520 - val_binary_accuracy: 0.7884\n",
      "Epoch 7/30\n",
      "5830/5830 [==============================] - 1s 106us/sample - loss: 2.8104 - binary_accuracy: 0.8802 - val_loss: 4.8766 - val_binary_accuracy: 0.7531\n",
      "Epoch 8/30\n",
      "5830/5830 [==============================] - 1s 103us/sample - loss: 2.6720 - binary_accuracy: 0.8903 - val_loss: 4.8317 - val_binary_accuracy: 0.7884\n",
      "Epoch 9/30\n",
      "5830/5830 [==============================] - 1s 105us/sample - loss: 2.5329 - binary_accuracy: 0.9027 - val_loss: 5.1403 - val_binary_accuracy: 0.7852\n",
      "Epoch 10/30\n",
      "5830/5830 [==============================] - 1s 106us/sample - loss: 2.4448 - binary_accuracy: 0.9110 - val_loss: 5.1322 - val_binary_accuracy: 0.8012\n",
      "Epoch 11/30\n",
      "5830/5830 [==============================] - 1s 103us/sample - loss: 2.3790 - binary_accuracy: 0.9148 - val_loss: 5.1833 - val_binary_accuracy: 0.8019\n",
      "Epoch 12/30\n",
      "5830/5830 [==============================] - 1s 105us/sample - loss: 2.3171 - binary_accuracy: 0.9194 - val_loss: 5.7396 - val_binary_accuracy: 0.8028\n",
      "Epoch 13/30\n",
      "5830/5830 [==============================] - 1s 114us/sample - loss: 2.2761 - binary_accuracy: 0.9245 - val_loss: 6.0835 - val_binary_accuracy: 0.8008\n",
      "Epoch 14/30\n",
      "5830/5830 [==============================] - 1s 112us/sample - loss: 2.2394 - binary_accuracy: 0.9268 - val_loss: 6.2686 - val_binary_accuracy: 0.8019\n",
      "Epoch 15/30\n",
      "5830/5830 [==============================] - 1s 106us/sample - loss: 2.2122 - binary_accuracy: 0.9304 - val_loss: 6.1584 - val_binary_accuracy: 0.8043\n",
      "Epoch 16/30\n",
      "5830/5830 [==============================] - 1s 112us/sample - loss: 2.1808 - binary_accuracy: 0.9328 - val_loss: 6.4942 - val_binary_accuracy: 0.8005\n",
      "Epoch 17/30\n",
      "5830/5830 [==============================] - 1s 110us/sample - loss: 2.1650 - binary_accuracy: 0.9340 - val_loss: 6.7608 - val_binary_accuracy: 0.8035\n",
      "Epoch 18/30\n",
      "5830/5830 [==============================] - 1s 106us/sample - loss: 2.1320 - binary_accuracy: 0.9358 - val_loss: 6.5757 - val_binary_accuracy: 0.7920\n",
      "Epoch 19/30\n",
      "5830/5830 [==============================] - 1s 105us/sample - loss: 2.1204 - binary_accuracy: 0.9383 - val_loss: 7.3062 - val_binary_accuracy: 0.7992\n",
      "Epoch 20/30\n",
      "5830/5830 [==============================] - 1s 107us/sample - loss: 2.1082 - binary_accuracy: 0.9385 - val_loss: 7.5778 - val_binary_accuracy: 0.7937\n",
      "Epoch 21/30\n",
      "5830/5830 [==============================] - 1s 110us/sample - loss: 2.0791 - binary_accuracy: 0.9415 - val_loss: 8.6214 - val_binary_accuracy: 0.7960\n",
      "Epoch 22/30\n",
      "5830/5830 [==============================] - 1s 105us/sample - loss: 2.0936 - binary_accuracy: 0.9403 - val_loss: 7.9370 - val_binary_accuracy: 0.7998\n",
      "Epoch 23/30\n",
      "5830/5830 [==============================] - 1s 109us/sample - loss: 2.0769 - binary_accuracy: 0.9420 - val_loss: 7.8970 - val_binary_accuracy: 0.8000\n",
      "Epoch 24/30\n",
      "5830/5830 [==============================] - 1s 115us/sample - loss: 2.0638 - binary_accuracy: 0.9430 - val_loss: 8.5258 - val_binary_accuracy: 0.7809\n",
      "Epoch 25/30\n",
      "5830/5830 [==============================] - 1s 108us/sample - loss: 2.0598 - binary_accuracy: 0.9439 - val_loss: 8.2122 - val_binary_accuracy: 0.7722\n",
      "Epoch 26/30\n",
      "5830/5830 [==============================] - 1s 106us/sample - loss: 2.0470 - binary_accuracy: 0.9447 - val_loss: 8.5784 - val_binary_accuracy: 0.7872\n",
      "Epoch 27/30\n",
      "5830/5830 [==============================] - 1s 105us/sample - loss: 2.0507 - binary_accuracy: 0.9448 - val_loss: 8.8280 - val_binary_accuracy: 0.7997\n",
      "Epoch 28/30\n",
      "5830/5830 [==============================] - 1s 106us/sample - loss: 2.0483 - binary_accuracy: 0.9456 - val_loss: 8.9767 - val_binary_accuracy: 0.7998\n",
      "Epoch 29/30\n",
      "5830/5830 [==============================] - 1s 103us/sample - loss: 2.0451 - binary_accuracy: 0.9453 - val_loss: 9.2408 - val_binary_accuracy: 0.7965\n",
      "Epoch 30/30\n",
      "5830/5830 [==============================] - 1s 105us/sample - loss: 2.0452 - binary_accuracy: 0.9461 - val_loss: 9.2459 - val_binary_accuracy: 0.7965\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=42, test_size=0.1)\n",
    "\n",
    "batch_size = 64  # mini-batch with 256 examples\n",
    "epochs = 30\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0   243    21     7 14994     8\n",
      "  1013    48  3885    58  3176  9180     9 14995  1300 14996     9 14997\n",
      " 14998  1300     9  8708     8  1013    48  3008  1811    36   214   115\n",
      "    39   657  8838     9   276     5  6499     2 14999   490     6   385\n",
      " 15000     8  1013    48   103    11   308  3559]\n"
     ]
    }
   ],
   "source": [
    "print(x_test[1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9839dc81feffa0ef6b0305e2f8d6f48bfea3278be234a4dfafb21f6ff94f6b4b"
  },
  "kernelspec": {
   "display_name": "Python 3.6.14 64-bit ('3.6.14')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
